# -*- coding: utf-8 -*-
"""Untitled4-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BVk05Lj83zFfGB48mIilE_oOZVrM632f
"""

import gdown

print("---------------------------------Started Downloading Dataset---------------------------------")
url = "https://drive.google.com/uc?id=1_xadMaT0gBeDpJwvIDnXNkxRA1-8Mx6P"
output = "data.zip"
gdown.download(url, output, quiet=False)

import zipfile
with zipfile.ZipFile(output, 'r') as zip_ref:
    zip_ref.extractall()

print("---------------------------------Completed Downloading Dataset---------------------------------")
import PIL.Image as Image
import matplotlib.pylab as plt
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import os 
import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential,Model
from keras.layers import Dense,Conv2D, MaxPooling2D, BatchNormalization,Dropout, Flatten,Activation,concatenate,Input,AlphaDropout
import tensorflow as tf
import random as rn
import cv2                  
from tqdm import tqdm
from random import shuffle  
import itertools
from keras.callbacks import EarlyStopping
from keras import backend as K

if not os.path.exists("outputs/waterBottleDataset"):
    os.makedirs("outputs/waterBottleDataset")


img_size = 224
batch_size = 32

datagen = ImageDataGenerator(rescale=1/255.,
                            zoom_range=0.2,
                            horizontal_flip=True,
                            validation_split=0.15)

train_generator = datagen.flow_from_directory('archive',
                                              target_size=(img_size,img_size),
                                              batch_size=batch_size,
                                              shuffle=True,
                                              subset='training',
                                              #color_mode='grayscale',
                                              class_mode='categorical')

val_generator = datagen.flow_from_directory('archive',
                                            target_size=(img_size,img_size),
                                            batch_size=batch_size,
                                            shuffle=False,
                                            subset='validation',
                                            #color_mode='grayscale',
                                            class_mode='categorical')

label = [k for k in train_generator.class_indices]
samples = train_generator.__next__()
images = samples[0]
titles = samples[1]
plt.figure(figsize=(20,20))

for i in range(15):
    plt.subplot(5,5,i+1)
    plt.subplots_adjust(hspace=0.3,wspace=0.3)
    plt.imshow(images[i])
    plt.title(f"Class: {label[np.argmax(titles[i],axis=0)]}")
    plt.axis("off")
plt.savefig("outputs/waterBottleDataset/sampleData.png")
"""# VGG16"""
print("\n")
print("---------------------------------Creating VGG16 Model---------------------------------")


from tensorflow.keras.applications.vgg16 import VGG16
base_model=VGG16(include_top=False,weights='imagenet',input_shape=(img_size,img_size,3))
base_model.trainable = False 
from tensorflow.keras import layers, models
flatten_layer = layers.Flatten()
dense_layer_1 = layers.Dense(50, activation='relu')
dense_layer_2 = layers.Dense(20, activation='relu')
prediction_layer = layers.Dense(3, activation='softmax')


Vgg_model = models.Sequential([
    base_model,
    flatten_layer,
    dense_layer_1,
    dense_layer_2,
    prediction_layer
])
Vgg_model.summary()

print("---------------------------------Creating VGG16 Model---------------------------------")

print("--------------------------------- VGG16 Model with Adam Optimizer---------------------------------")

"""## Adam"""

beta1 = 0.9
beta2 = 0.99
epsilonE = 1e-06

vggadam=Vgg_model
from tensorflow.keras.callbacks import EarlyStopping
adam1 = tf.keras.optimizers.Adam(
    learning_rate = 0.001,
    beta_1 = beta1,
    beta_2 = beta2,
    epsilon = epsilonE,
    amsgrad=False,
 
)
vggadam.compile(
    optimizer=adam1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)

es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)

# , validation_split=0.2
historyvggadam = vggadam.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

vggadamval=vggadam.evaluate(val_generator)
vggadamval
print("\n")
print("VGG16 Model - Adam Optimizer Accuracy - ",vggadamval[1]*100 )

print("\n")

"""## ADA"""
print("--------------------------------- VGG16 Model with Adagrad Optimizer---------------------------------")

vggada=Vgg_model
adagrad1 = tf.keras.optimizers.Adagrad(
    learning_rate = 0.001,
    initial_accumulator_value = 0.1,
    epsilon = epsilonE,
)

vggada.compile(
    optimizer=adagrad1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
historyvggada = vggada.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

vggadaval=vggada.evaluate(val_generator)
print("\n")
print("VGG16 Model - Adagrad Optimizer Accuracy - ",vggadaval[1]*100 )

vggadaval
print("\n")
"""##RMS"""
print("--------------------------------- VGG16 Model with RMSProp Optimizer---------------------------------")

# RMSProp
vggrms = Vgg_model
rmsprop1 = tf.keras.optimizers.RMSprop(
    learning_rate = 0.001,
    rho = 0.9,
    momentum = 0.0,
    epsilon = epsilonE,
    centered=False,
)
vggrms.compile(
    optimizer=rmsprop1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
historyvggrms = vggrms.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

vggrmsval=vggrms.evaluate(val_generator)
print("\n")
print("VGG16 Model - RMSProp Optimizer Accuracy - ",vggrmsval[1]*100 )

vggrmsval
print("\n")
"""##SGD"""
print("--------------------------------- VGG16 Model with SGD Optimizer---------------------------------")

vggsgd = Vgg_model
momentum1 = tf.keras.optimizers.SGD(
    learning_rate = 0.01,
    momentum = beta1,
    nesterov = False,
)

vggsgd.compile(
    optimizer=momentum1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
historyvggsgd = vggsgd.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

vggsgdval=vggsgd.evaluate(val_generator)
print("\n")
print("VGG16 Model - SGD Optimizer Accuracy - ",vggsgdval[1]*100 )
print("\n")
vggsgdval

"""## Plots"""

import seaborn as sns
classifiers = [vggadamval , vggadaval ,vggrmsval ,vggsgdval]
cv_results_res = []
for i in classifiers :
    cv_results_res.append(i[1])
cv_results_res = pd.DataFrame({"Evaluate":cv_results_res,"Network":["vggAdam","vggAdagrad","vggRMS",
"vggSGD"]})

cv_results_res

# use seaborn to create a barplot
sns.set(style="whitegrid")
ax = sns.barplot(x="Network", y="Evaluate", data=cv_results_res)

# add labels to the plot
ax.set_title("Evaluation Scores for Different Optimizers for VGG16")
ax.set_xlabel("Network")
ax.set_ylabel("Accuracy")

# display the plot
plt.savefig("outputs/waterBottleDataset/evaluation_scores.png")
plt.show()

"""## Loss Plots"""

# Plot training & validation loss values
plt.plot(historyvggadam.history['loss'])
plt.plot(historyvggada.history['loss'])
plt.plot(historyvggrms.history['loss'])
plt.plot(historyvggsgd.history['loss'])
plt.title('VGG16 Model Training Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='upper right')
plt.savefig("outputs/waterBottleDataset/vgg16_training_loss.png")
plt.show()

# Plot training & validation loss values
plt.plot(historyvggadam.history['val_loss'])
plt.plot(historyvggada.history['val_loss'])
plt.plot(historyvggrms.history['val_loss'])
plt.plot(historyvggsgd.history['val_loss'])
plt.title('VGG16 Model Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='upper right')
plt.savefig("outputs/waterBottleDataset/vgg16_validation_loss.png")
plt.show()

"""## Accuracy Plots"""

# Plot training & validation loss values
plt.plot(historyvggadam.history['accuracy'])
plt.plot(historyvggada.history['accuracy'])
plt.plot(historyvggrms.history['accuracy'])
plt.plot(historyvggsgd.history['accuracy'])
plt.title('VGG16 Model Training Accuracy')
plt.ylabel('accuracy')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='lower right')
plt.savefig("outputs/waterBottleDataset/vgg16_training_accuracy.png")
plt.show()

# Plot training & validation loss values
plt.plot(historyvggadam.history['val_accuracy'])
plt.plot(historyvggada.history['val_accuracy'])
plt.plot(historyvggrms.history['val_accuracy'])
plt.plot(historyvggsgd.history['val_accuracy'])
plt.title('VGG16 Model Validation Accuracy')
plt.ylabel('accuracy')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='lower right')
plt.savefig("outputs/waterBottleDataset/vgg16_validation_accuracy.png")
plt.show()

"""## Whisker Plot"""

import seaborn as sns
import pandas as pd
optimizer =['Adam', 'Adagrad','RMSprop','SGD']
histories =[historyvggadam,historyvggada,historyvggrms,historyvggsgd]
acc_data=[]
for i in range(len(optimizer)):
    acc = histories[i].history['val_accuracy']
    acc_data.append(acc)
  
fig, ax = plt.subplots()
ax.boxplot(acc_data, labels=[str(i) for i in optimizer])
ax.set_ylabel('Accuracy')
ax.set_xlabel('Optimizers')
ax.set_title('VGG16 Model - Validation Accuracy by Optimizer')
plt.savefig("outputs/waterBottleDataset/vgg16_validation_accuracy_whiskerplot.png")
plt.show()

"""# ResNet50"""
print("---------------------------------  Strated ResNet50 Model ---------------------------------")

from tensorflow.keras.applications.resnet50 import ResNet50
base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(img_size, img_size, 3))
base_model.trainable = False

from tensorflow.keras import layers, models
flatten_layer = layers.Flatten()
dense_layer_1 = layers.Dense(50, activation='relu')
dense_layer_2 = layers.Dense(20, activation='relu')
prediction_layer = layers.Dense(3, activation='softmax')

ResNet_model = models.Sequential([
    base_model,
    flatten_layer,
    dense_layer_1,
    dense_layer_2,
    prediction_layer
])
ResNet_model.summary()

"""## Adam"""
print("--------------------------------- ResNet50 Model with Adam Optimizer---------------------------------")

beta1 = 0.9
beta2 = 0.99
epsilonE = 1e-06

resnet_adam = ResNet_model
adam1 = tf.keras.optimizers.Adam(
    learning_rate = 0.001,
    beta_1 = beta1,
    beta_2 = beta2,
    epsilon = epsilonE,
    amsgrad=False,
)

resnet_adam.compile(
    optimizer=adam1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
# , validation_split=0.2
history_resnet_adam = resnet_adam.fit(train_generator, epochs=50, validation_data=val_generator, batch_size=32, callbacks=[es])

resnet_adamval=resnet_adam.evaluate(val_generator)
resnet_adamval
print("\n")
print("ResNet Model - Adam Optimizer Accuracy - ",resnet_adamval[1]*100 )


"""##ADA"""
print("--------------------------------- ResNet50 Model with Adagrad Optimizer---------------------------------")

resnetada=ResNet_model
adagrad1 = tf.keras.optimizers.Adagrad(
    learning_rate = 0.001,
    initial_accumulator_value = 0.1,
    epsilon = epsilonE,
)

resnetada.compile(
    optimizer=adagrad1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
historyresnetada = resnetada.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

resnetadaval=resnetada.evaluate(val_generator)
print("\n")
print("ResNet Model - Adagrad Optimizer Accuracy - ",resnetadaval[1]*100 )

"""##RMSPROP"""
print("--------------------------------- ResNet50 Model with RMSProp Optimizer---------------------------------")

# RMSProp
resnetrms = ResNet_model
rmsprop1 = tf.keras.optimizers.RMSprop(
    learning_rate = 0.001,
    rho = 0.9,
    momentum = 0.0,
    epsilon = epsilonE,
    centered=False,
)
resnetrms.compile(
    optimizer=rmsprop1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
historyresnetrms = resnetrms.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

resnetrmsval=resnetrms.evaluate(val_generator)
print("\n")
print("ResNet Model - RMSProp Optimizer Accuracy - ",resnetrmsval[1]*100 )

"""##SGD"""
print("--------------------------------- ResNet50 Model with SGD Optimizer---------------------------------")

resnetsgd = ResNet_model
momentum1 = tf.keras.optimizers.SGD(
    learning_rate = 0.01,
    momentum = beta1,
    nesterov = False,
)

resnetsgd.compile(
    optimizer=momentum1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
historyresnetsgd = resnetsgd.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

resnetsgdval=resnetsgd.evaluate(val_generator)
print("\n")
print("VGG16 Model - SGD Optimizer Accuracy - ",resnetsgdval[1]*100 )

"""## Plots"""

import seaborn as sns
classifiers = [resnet_adamval , resnetadaval ,resnetrmsval ,resnetsgdval]
cv_results_res = []
for i in classifiers :
    cv_results_res.append(i[1])
cv_results_res = pd.DataFrame({"Evaluate":cv_results_res,"Network":["resAdam","resAdagrad","resRMS",
"resSGD"]})

cv_results_res

# use seaborn to create a barplot
sns.set(style="whitegrid")
ax = sns.barplot(x="Network", y="Evaluate", data=cv_results_res)

# add labels to the plot
ax.set_title("Evaluation Scores for Different Optimizers for ResNet50")
ax.set_xlabel("Network")
ax.set_ylabel("Accuracy")
plt.savefig("outputs/waterBottleDataset/resnet50_evaluation_scores.png")
# display the plot
plt.show()

"""## Accuracy Plots"""

# Plot training & validation loss values
plt.plot(history_resnet_adam.history['accuracy'])
plt.plot(historyresnetada.history['accuracy'])
plt.plot(historyresnetrms.history['accuracy'])
plt.plot(historyresnetsgd.history['accuracy'])
plt.title('ResNet50 Model Training Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='lower right')
plt.savefig("outputs/waterBottleDataset/resnet50_training_accuracy.png")
plt.show()

# Plot training & validation loss values
plt.plot(history_resnet_adam.history['val_accuracy'])
plt.plot(historyresnetada.history['val_accuracy'])
plt.plot(historyresnetrms.history['val_accuracy'])
plt.plot(historyresnetsgd.history['val_accuracy'])
plt.title('ResNet50 Model Validation Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='lower right')
plt.savefig("outputs/waterBottleDataset/resnet50_validation_accuracy.png")
plt.show()

"""## Loss Plots"""

# Plot training & validation loss values
plt.plot(history_resnet_adam.history['loss'])
plt.plot(historyresnetada.history['loss'])
plt.plot(historyresnetrms.history['loss'])
plt.plot(historyresnetsgd.history['loss'])
plt.title('ResNet50 Model Training Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='upper right')
plt.savefig("outputs/waterBottleDataset/resnet50_training_loss.png")
plt.show()

# Plot training & validation loss values
plt.plot(history_resnet_adam.history['val_loss'])
plt.plot(historyresnetada.history['val_loss'])
plt.plot(historyresnetrms.history['val_loss'])
plt.plot(historyresnetsgd.history['val_loss'])
plt.title('ResNet50 Model Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='upper right')
plt.savefig("outputs/waterBottleDataset/resnet50_validation_loss.png")
plt.show()

"""## Whisker Plot"""

import seaborn as sns
import pandas as pd
optimizer =['Adam', 'Adagrad','RMSprop','SGD']
histories =[history_resnet_adam,historyresnetada,historyresnetrms,historyresnetsgd]
acc_data=[]
for i in range(len(optimizer)):
    acc = histories[i].history['val_accuracy']
    acc_data.append(acc)
  
fig, ax = plt.subplots()
ax.boxplot(acc_data, labels=[str(i) for i in optimizer])
ax.set_ylabel('Accuracy')
ax.set_xlabel('Optimizers')
ax.set_title('ResNet50 Model - Validation Accuracy by Optimizer')
plt.savefig("outputs/waterBottleDataset/resnet50_validation_accuracy_whiskerplot.png")
plt.show()

"""# InceptionResNetV2"""
print("--------------------------------- InceptionResNetV2 Model ---------------------------------")

from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2
base_model = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(img_size, img_size, 3))
base_model.trainable = False

flatten_layer = layers.Flatten()
dense_layer_1 = layers.Dense(50, activation='relu')
dense_layer_2 = layers.Dense(20, activation='relu')
prediction_layer = layers.Dense(3, activation='softmax')

InceptionResNet_model = models.Sequential([
    base_model,
    flatten_layer,
    dense_layer_1,
    dense_layer_2,
    prediction_layer
])
InceptionResNet_model.summary()

beta1 = 0.9
beta2 = 0.99
epsilonE = 1e-06

"""## Adam"""
print("--------------------------------- InceptionResNetV2 Model with Adam Optimizer---------------------------------")
print("\n")
inception_resnet_adam = InceptionResNet_model
from tensorflow.keras.callbacks import EarlyStopping
adam1 = tf.keras.optimizers.Adam(
    learning_rate = 0.001,
    beta_1 = beta1,
    beta_2 = beta2,
    epsilon = epsilonE,
    amsgrad=False,
)

inception_resnet_adam.compile(
    optimizer=adam1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
# , validation_split=0.2
history_inception_resnet_adam = inception_resnet_adam.fit(train_generator, epochs=50, validation_data=val_generator, batch_size=32, callbacks=[es])

inception_resnet_adamval=inception_resnet_adam.evaluate(val_generator)
print("\n")
print("InceptionResNetV2 Model - Adam Optimizer Accuracy - ",inception_resnet_adamval[1]*100 )
inception_resnet_adam

"""##ADA"""
print("\n")
print("--------------------------------- InceptionResNetV2 Model with Adagrad Optimizer---------------------------------")
print("\n")
inception_resnet_ada=InceptionResNet_model
adagrad1 = tf.keras.optimizers.Adagrad(
    learning_rate = 0.001,
    initial_accumulator_value = 0.1,
    epsilon = epsilonE,
)

inception_resnet_ada.compile(
    optimizer=adagrad1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
history_inception_resnet_ada = inception_resnet_ada.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

inception_resnet_adaval=inception_resnet_ada.evaluate(val_generator)
print("\n")
print("InceptionResNetV2 Model - Adagrad Optimizer Accuracy - ",inception_resnet_adaval[1]*100 )
inception_resnet_adaval

"""##RMSPROP"""
print("\n")
print("--------------------------------- InceptionResNetV2 Model with RMSProp Optimizer---------------------------------")
print("\n")
# RMSProp
inception_resnet_rms = InceptionResNet_model
rmsprop1 = tf.keras.optimizers.RMSprop(
    learning_rate = 0.001,
    rho = 0.9,
    momentum = 0.0,
    epsilon = epsilonE,
    centered=False,
)
inception_resnet_rms.compile(
    optimizer=rmsprop1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
historyresnetrms = inception_resnet_rms.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

inception_resnet_rmsval=inception_resnet_rms.evaluate(val_generator)
print("\n")
print("InceptionResNetV2 Model - RMSProp Optimizer Accuracy - ",inception_resnet_rmsval[1]*100 )
inception_resnet_rmsval

"""## SGD"""
print("\n")
print("--------------------------------- InceptionResNetV2 Model with SGD Optimizer---------------------------------")
print("\n")
inception_resnet_sgd = InceptionResNet_model
momentum1 = tf.keras.optimizers.SGD(
    learning_rate = 0.01,
    momentum = beta1,
    nesterov = False,
)

inception_resnet_sgd.compile(
    optimizer=momentum1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
historyinception_resnet_sgd = inception_resnet_sgd.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

inception_resnet_sgdval=inception_resnet_sgd.evaluate(val_generator)
print("\n")
print("InceptionResNetV2 Model - SGD Optimizer Accuracy - ",inception_resnet_sgdval[1]*100 )
inception_resnet_sgdval

"""## Plots"""

import seaborn as sns
classifiers = [inception_resnet_adamval , inception_resnet_adaval ,inception_resnet_rmsval ,inception_resnet_sgdval]
cv_results_res = []
for i in classifiers :
    cv_results_res.append(i[1])
cv_results_res = pd.DataFrame({"Evaluate":cv_results_res,"Network":["incresAdam","incresAdagrad","incresRMS",
"incresSGD"]})

cv_results_res

# use seaborn to create a barplot
sns.set(style="whitegrid")
ax = sns.barplot(x="Network", y="Evaluate", data=cv_results_res)

# add labels to the plot
ax.set_title("Evaluation Scores for Different Optimizers for InceptionResNet50")
ax.set_xlabel("Network")
ax.set_ylabel("Accuracy")
plt.savefig("outputs/waterBottleDataset/inceptionresnet50_evaluation_scores.png")
# display the plot
plt.show()

"""## Loss Plots"""

# Plot training & validation loss values
plt.plot(history_inception_resnet_adam.history['loss'])
plt.plot(history_inception_resnet_ada.history['loss'])
plt.plot(historyresnetrms.history['loss'])
plt.plot(historyinception_resnet_sgd.history['loss'])
plt.title('InceptionResNet50 Model Training Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='upper right')
plt.savefig("outputs/waterBottleDataset/inceptionresnet50_training_loss.png")
plt.show()

# Plot training & validation loss values
plt.plot(history_inception_resnet_adam.history['val_loss'])
plt.plot(history_inception_resnet_ada.history['val_loss'])
plt.plot(historyresnetrms.history['val_loss'])
plt.plot(historyinception_resnet_sgd.history['val_loss'])
plt.title('InceptionResNet50 Model Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='upper right')
plt.savefig("outputs/waterBottleDataset/inceptionresnet50_validation_loss.png")
plt.show()

"""## Accuracy Plots"""

# Plot training & validation loss values
plt.plot(history_inception_resnet_adam.history['accuracy'])
plt.plot(history_inception_resnet_ada.history['accuracy'])
plt.plot(historyresnetrms.history['accuracy'])
plt.plot(historyinception_resnet_sgd.history['accuracy'])
plt.title('InceptionResNet50 Model Training Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='lower right')
plt.savefig("outputs/waterBottleDataset/inceptionresnet50_training_accuracy.png")
plt.show()

# Plot training & validation loss values
plt.plot(history_inception_resnet_adam.history['val_accuracy'])
plt.plot(history_inception_resnet_ada.history['val_accuracy'])
plt.plot(historyresnetrms.history['val_accuracy'])
plt.plot(historyinception_resnet_sgd.history['val_accuracy'])
plt.title('InceptionResNet50 Model Validation Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='lower right')
plt.savefig("outputs/waterBottleDataset/inceptionresnet50_validation_accuracy.png")
plt.show()

"""## Whisker Plot"""

import seaborn as sns
import pandas as pd
optimizer =['Adam', 'Adagrad','RMSprop','SGD']
histories =[history_inception_resnet_adam,history_inception_resnet_ada,historyresnetrms,historyinception_resnet_sgd]
acc_data=[]
for i in range(len(optimizer)):
    acc = histories[i].history['val_accuracy']
    acc_data.append(acc)
  
fig, ax = plt.subplots()
ax.boxplot(acc_data, labels=[str(i) for i in optimizer])
ax.set_ylabel('Accuracy')
ax.set_xlabel('Optimizers')
ax.set_title('InceptionResNet50 Model - Validation Accuracy by Optimizer')
plt.savefig("outputs/waterBottleDataset/inceptionresnet50_val_acc_whiskerplot.png")
plt.show()





"""# Xception"""
print("--------------------------------- Started Xception Model ---------------------------------")

from tensorflow.keras.applications.xception import Xception
base_model = Xception(include_top=False, weights='imagenet', input_shape=(img_size, img_size, 3))
base_model.trainable = False

from tensorflow.keras import layers, models
flatten_layer = layers.Flatten()
dense_layer_1 = layers.Dense(50, activation='relu')
dense_layer_2 = layers.Dense(20, activation='relu')
prediction_layer = layers.Dense(3, activation='softmax')

Xception_model = models.Sequential([
    base_model,
    flatten_layer,
    dense_layer_1,
    dense_layer_2,
    prediction_layer
])
Xception_model.summary()

beta1 = 0.9
beta2 = 0.99
epsilonE = 1e-06

"""## Adam"""
print("--------------------------------- Xception Model with Adam Optimizer---------------------------------")

xception_adam = Xception_model
from tensorflow.keras.callbacks import EarlyStopping
adam1 = tf.keras.optimizers.Adam(
    learning_rate = 0.001,
    beta_1 = beta1,
    beta_2 = beta2,
    epsilon = epsilonE,
    amsgrad=False,
)

xception_adam.compile(
    optimizer=adam1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
# , validation_split=0.2
history_xception_adam = xception_adam.fit(train_generator, epochs=50, validation_data=val_generator, batch_size=32, callbacks=[es])

xception_adamval=xception_adam.evaluate(val_generator)
print("\n")
print("Xception Model - Adam Optimizer Accuracy - ",xception_adamval[1]*100 )
xception_adamval

"""##ADA"""
print("\n")
print("--------------------------------- Xception Model with Adagrad Optimizer---------------------------------")

xception_ada=Xception_model
adagrad1 = tf.keras.optimizers.Adagrad(
    learning_rate = 0.001,
    initial_accumulator_value = 0.1,
    epsilon = epsilonE,
)

xception_ada.compile(
    optimizer=adagrad1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
history_xception_ada = xception_ada.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

xception_adaval=xception_ada.evaluate(val_generator)
print("\n")
print("Xception Model - Adagrad Optimizer Accuracy - ",xception_adaval[1]*100 )
xception_adaval

"""##RMSPROP"""
print("\n")
print("--------------------------------- Xception Model with RMSProp Optimizer---------------------------------")

# RMSProp
xception_rms = Xception_model
rmsprop1 = tf.keras.optimizers.RMSprop(
    learning_rate = 0.001,
    rho = 0.9,
    momentum = 0.0,
    epsilon = epsilonE,
    centered=False,
)
xception_rms.compile(
    optimizer=rmsprop1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
historyxceptionrms = xception_rms.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

xception_rmsval=xception_rms.evaluate(val_generator)
print("\n")
print("Xception Model - RMSProp Optimizer Accuracy - ",xception_rmsval[1]*100 )
xception_rmsval

"""##SGD"""
print("\n")
print("--------------------------------- Xception Model with SGD Optimizer---------------------------------")

xception_sgd = Xception_model
momentum1 = tf.keras.optimizers.SGD(
    learning_rate = 0.01,
    momentum = beta1,
    nesterov = False,
)

xception_sgd.compile(
    optimizer=momentum1,
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
historyxception_sgd = xception_sgd.fit(train_generator, epochs=50,validation_data=val_generator, batch_size=32, callbacks=[es])

xception_sgdval=xception_sgd.evaluate(val_generator)
print("\n")
print("Xception Model - SGD Optimizer Accuracy - ",xception_sgdval[1]*100 )
xception_sgdval

"""## Plots"""

import seaborn as sns
classifiers = [xception_adamval , xception_adaval ,xception_rmsval ,xception_sgdval]
cv_results_res = []
for i in classifiers :
    cv_results_res.append(i[1])
cv_results_res = pd.DataFrame({"Evaluate":cv_results_res,"Network":["xceptionAdam","xceptionAdagrad","xceptionRMS",
"xceptionSGD"]})

cv_results_res

# use seaborn to create a barplot
sns.set(style="whitegrid")
ax = sns.barplot(x="Network", y="Evaluate", data=cv_results_res)

# add labels to the plot
ax.set_title("Evaluation Scores for Different Optimizers for Xception")
ax.set_xlabel("Network")
ax.set_ylabel("Accuracy")
plt.savefig("outputs/waterBottleDataset/xception_optimizer_evaluation.png")
# display the plot
plt.show()

"""## Loss Plots"""

# Plot training & validation loss values
plt.plot(history_xception_adam.history['loss'])
plt.plot(history_xception_ada.history['loss'])
plt.plot(historyxceptionrms.history['loss'])
plt.plot(historyxception_sgd.history['loss'])
plt.title('Xception Model Training Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='upper right')
plt.savefig("outputs/waterBottleDataset/xception_training_loss.png")
plt.show()

# Plot training & validation loss values
plt.plot(history_xception_adam.history['val_loss'])
plt.plot(history_xception_ada.history['val_loss'])
plt.plot(historyxceptionrms.history['val_loss'])
plt.plot(historyxception_sgd.history['val_loss'])
plt.title('Xception Model Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='upper right')
plt.savefig("outputs/waterBottleDataset/xception_validation_loss.png")
plt.show()

"""## Accuracy Plot"""

# Plot training & validation loss values
plt.plot(history_xception_adam.history['accuracy'])
plt.plot(history_xception_ada.history['accuracy'])
plt.plot(historyxceptionrms.history['accuracy'])
plt.plot(historyxception_sgd.history['accuracy'])
plt.title('Xception Model Training Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='lower right')
plt.savefig("outputs/waterBottleDataset/xception_training_accuracy.png")
plt.show()

# Plot training & validation loss values
plt.plot(history_xception_adam.history['val_accuracy'])
plt.plot(history_xception_ada.history['val_accuracy'])
plt.plot(historyxceptionrms.history['val_accuracy'])
plt.plot(historyxception_sgd.history['val_accuracy'])
plt.title('Xception Model Validation Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Adam', 'Adagrad','RMSprop','SGD'], loc='lower right')
plt.savefig("outputs/waterBottleDataset/xception_validation_accuracy.png")
plt.show()

"""## Whisker Plot"""

import seaborn as sns
import pandas as pd
optimizer =['Adam', 'Adagrad','RMSprop','SGD']
histories =[history_xception_adam,history_xception_ada,historyxceptionrms,historyxception_sgd]
acc_data=[]
for i in range(len(optimizer)):
    acc = histories[i].history['val_accuracy']
    acc_data.append(acc)
  
fig, ax = plt.subplots()
ax.boxplot(acc_data, labels=[str(i) for i in optimizer])
ax.set_ylabel('Accuracy')
ax.set_xlabel('Optimizers')
ax.set_title('Xception Model - Validation Accuracy by Optimizer')
plt.savefig("outputs/waterBottleDataset/xception_val_acc_whiskerplot.png")
plt.show()

